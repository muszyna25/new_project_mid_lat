{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed random-search hyper-parameter optimization of the CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "\n",
    "# External imports\n",
    "import ipyparallel as ipp\n",
    "import numpy as np\n",
    "import tensorflow.python.keras\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Local imports\n",
    "from model_code import get_job_id, save_hyper_params_to_csv\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def setup_cluster(model_name):\n",
    "    # Cluster ID taken from job ID above\n",
    "    job_id = get_job_id('squeue -u muszyng -o %A -n ' + model_name)\n",
    "    cluster_id = 'cori_{}'.format(job_id)\n",
    "    print('Job id: %d; Cluster id: %s' %(job_id, cluster_id))\n",
    "\n",
    "    # Use default profile\n",
    "    c = ipp.Client(timeout=60, cluster_id=cluster_id)\n",
    "    print('Worker IDs:', c.ids)\n",
    "    \n",
    "    return job_id, cluster_id, c\n",
    "    \n",
    "def get_train_val_names(input_dir):\n",
    "    l_fnames_train = [os.path.basename(x) for x in glob.glob(input_dir + 'train*.h5')]\n",
    "    l_fnames_val = [os.path.basename(x) for x in glob.glob(input_dir + 'val*.h5')] \n",
    "    print(list(l_fnames_train), list(l_fnames_val), sep='\\n')\n",
    "    return l_fnames_train, l_fnames_val\n",
    "\n",
    "def build_and_train(input_dir, input_size,\n",
    "                    conv_sizes, fc_sizes, dropout, optimizer, lr,\n",
    "                    batch_size, n_epochs, repeats_conv, recep_field, \n",
    "                    channels, checkpoint_dir=None, checkpoint_file=None, \n",
    "                    l_fnames_train=None, l_fnames_val=None, ihp=0, verbose=2):\n",
    "    \n",
    "    \"\"\"Run training for one set of hyper-parameters\"\"\"\n",
    "    import sys\n",
    "    from model_code import build_model, train_model\n",
    "    from mlextras import configure_session\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    from contextlib import redirect_stdout\n",
    "\n",
    "    def save_model_summary(i, dir_path):\n",
    "        with open(dir_path + '/modelsummary_' + str(i) + '.csv', 'w') as f:\n",
    "            with redirect_stdout(f):\n",
    "                model.summary()\n",
    "                n_params = model.count_params()/1000.\n",
    "                print('No of parameters: %i' %n_params)\n",
    "            \n",
    "    def save_model_config(i, dir_path):\n",
    "        model_yaml = model.to_yaml()\n",
    "        with open(dir_path + '/model_' + str(i) + '.yaml', 'w') as yaml_file:\n",
    "            yaml_file.write(model_yaml)\n",
    "    \n",
    "    change_model_dbg = True\n",
    "    \n",
    "    oldStdout = sys.stdout\n",
    "    file = open(checkpoint_dir + '/logFile_' + str(ihp) + '.log', 'w')\n",
    "    sys.stdout = file\n",
    "\n",
    "    import tensorflow.python.keras\n",
    "\n",
    "    print('Hyperparameter trial %i, conv %s, fc %s, dropout %.4f, opt %s, lr %.4f, batch size %d, repeats conv layers %s, recep fields %s, checkpoint dir: %s' %\n",
    "          (ihp, conv_sizes, fc_sizes, dropout, optimizer, lr, batch_size, repeats_conv, recep_field, checkpoint_dir))\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Thread settings\n",
    "    tensorflow.keras.backend.set_session(configure_session())\n",
    "    \n",
    "    # Build the model\n",
    "    print(fc_sizes)\n",
    "    model = build_model(input_size,\n",
    "                        conv_sizes=conv_sizes, repeats_conv=repeats_conv, recep_field=recep_field, fc_sizes=fc_sizes,\n",
    "                        dropout=dropout, optimizer=optimizer, lr=lr)\n",
    "    \n",
    "    n_params = model.count_params()/1000.\n",
    "    model.summary()\n",
    "    print('No of parameters: %i' %n_params)\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    sys.stdout = oldStdout\n",
    "    \n",
    "    save_model_config(ihp, checkpoint_dir)\n",
    "    save_model_summary(ihp, checkpoint_dir)\n",
    "    \n",
    "    # Train the model\n",
    "    sys.stdout.flush()\n",
    "    start = time.time()\n",
    "    \n",
    "    if change_model_dbg:\n",
    "        history = train_model(model,\n",
    "                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                              checkpoint_dir=checkpoint_dir, checkpoint_file=checkpoint_file, \n",
    "                              data_dir_path=input_dir[:-1],\n",
    "                              l_fnames_train=l_fnames_train, l_fnames_val=l_fnames_val, ihp=ihp, verbose=verbose)\n",
    "        \n",
    "        oldStdout = sys.stdout\n",
    "        sys.stdout = file\n",
    "        print('train_model done, elaT=%.1f sec'%(time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout = oldStdout\n",
    "        return history.history\n",
    "\n",
    "def test_model_config(input_size, conv_sizes, repeats_conv, \n",
    "                      recep_field, fc_sizes,\n",
    "                      dropout, optimizer, lr):\n",
    "    \n",
    "    from model_code import build_model\n",
    "    \n",
    "    model = build_model(input_size, conv_sizes=conv_sizes, repeats_conv=repeats_conv, \n",
    "                         recep_field=recep_field, fc_sizes=fc_sizes,\n",
    "                         dropout=dropout, optimizer=optimizer, lr=lr)\n",
    "    model.summary()\n",
    "\n",
    "def plot_walltime(results):\n",
    "    done_results = [ar for ar in results if ar.ready()]\n",
    "    times = [(ar.completed - ar.started).total_seconds()/60.0 for ar in done_results]\n",
    "    plt.figure()\n",
    "    h = plt.hist(times, bins='auto', rwidth=0.3)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Training time [min]')\n",
    "    plt.ylabel('# of models');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_0'\n",
    "job_id, cluster_id, c = setup_cluster(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "input_dir = '/global/cscratch1/sd/muszyng/ethz_data/project_atmo_block_datasets/generated_datasets/generated_data_full_analysis/simplified_0/u/big_hdf5_files/chunk_files/reduced_chunk_files/'\n",
    "l_fnames_train, l_fnames_val = get_train_val_names(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the hyper-parameter search tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily making things reproducible for development\n",
    "#np.random.seed(0)\n",
    "\n",
    "# Define the hyper-parameter search points\n",
    "n_hpo_trials = 30\n",
    "\n",
    "n_convs_layers = 5\n",
    "layer_sizes = np.random.choice(np.array([8, 16, 32, 64]), size=(n_hpo_trials, n_convs_layers))\n",
    "conv_sizes = np.array([sorted(x, reverse=False) for x in layer_sizes])\n",
    "\n",
    "n_fc_layers = 3\n",
    "dense_sizes = np.array([512, 256, 128, 64])\n",
    "dtuple_sizes = np.random.choice(dense_sizes, size=(n_hpo_trials, n_fc_layers))\n",
    "fc_sizes = np.array([sorted(x, reverse=True) for x in dtuple_sizes])\n",
    "\n",
    "lr = np.zeros((n_hpo_trials,1))\n",
    "dropout = np.random.uniform(0.1, 0.5, size=n_hpo_trials)\n",
    "optimizer = np.random.choice(['Adam', 'Nadam'], size=n_hpo_trials)\n",
    "batch_sizes = np.random.choice([64, 128, 256], size=n_hpo_trials)\n",
    "\n",
    "repeats_conv = [(0,0)] #[(2,2), (3,2), (4,2)]\n",
    "recep_fields = (3,3)\n",
    "channels = (0,0)\n",
    "\n",
    "input_size = (60, 120, 8)\n",
    "\n",
    "# Training config\n",
    "n_epochs = 300\n",
    "checkpoint_dir = os.path.join(os.environ['SCRATCH'],'cnn_model_runs/%s/%i' % (model_name, job_id))\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hyper_params_to_csv(n_hpo_trials, n_convs_layers, conv_sizes, \n",
    "                         n_fc_layers, fc_sizes, dropout, \n",
    "                         optimizer, lr, batch_sizes, \n",
    "                         n_epochs, repeats_conv, recep_fields, \n",
    "                         checkpoint_dir, job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load-balanced view\n",
    "lv = c.load_balanced_view()\n",
    "\n",
    "# Loop over hyper-parameter sets\n",
    "results = []\n",
    "\n",
    "for ihp in range(n_hpo_trials):\n",
    "    \n",
    "    print('Hyperparameter trial %i conv %s fc %s dropout %.4f opt %s, lr %.4f, batch sizes %d' %\n",
    "          (ihp, conv_sizes[ihp], fc_sizes[ihp], dropout[ihp], optimizer[ihp], lr[ihp], batch_sizes[ihp]))\n",
    "    \n",
    "    checkpoint_file = os.path.join(checkpoint_dir, 'model_%i.h5' % ihp)\n",
    "    \n",
    "    print('Checkpoint file: %s' %checkpoint_file)\n",
    "    \n",
    "    result = lv.apply(build_and_train, input_dir, input_size, conv_sizes=conv_sizes[ihp], \n",
    "                      fc_sizes=fc_sizes[ihp], dropout=dropout[ihp], optimizer=optimizer[ihp], \n",
    "                      lr=lr[ihp], batch_size=batch_sizes[ihp], n_epochs=n_epochs, \n",
    "                      repeats_conv=repeats_conv, recep_field=recep_fields, channels=channels, \n",
    "                      checkpoint_dir=checkpoint_dir, checkpoint_file=checkpoint_file, l_fnames_train=l_fnames_train, \n",
    "                      l_fnames_val=l_fnames_val, ihp=ihp)\n",
    "    \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check workers status and walltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print standard out for one of the runs\n",
    "model_ID = random.randint(0, n_hpo_trials-1)\n",
    "print(model_ID)\n",
    "print(results[model_ID].stdout)\n",
    "print(results[model_ID].stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks how many tasks have been completed.\n",
    "print('Tasks completed: %i / %i' % (np.sum([ar.ready() for ar in results]), len(results)))\n",
    "print('Tasks status of all tasks: %s' % ([str(results[i].status) + ' ' + str(i) for i in range(0, len(results))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the results from all of the runs that have finished\n",
    "histories = [ar.get() if ar.ready() else None for ar in results]\n",
    "histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_walltime(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_config(input_size, conv_sizes[0], repeats_conv[0], recep_fields[0], fc_sizes[0], dropout[0], optimizer[0], lr[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-intel(cpu)/1.13.1-py36",
   "language": "python",
   "name": "tensorflow_intel_1.13.1_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
